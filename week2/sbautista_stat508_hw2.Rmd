---
title: "STAT 508 Data Analysis Assignment 2"
author: "Sebastian Bautista"
output:
  html_document:
    code_folding: hide
    df_print: paged
  html_notebook:
    code_folding: hide
    fig_caption: yes
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment='>')
```

# 0. Introduction

This week we’re looking at the `Carseats` data from the ISLR package. It’s a simulated cross-sectional data set containing sales of child car seats at 400 different stores and other variables about the characteristics of the communities surrounding the stores, competitor and sales prices, and other variables relevant to car seat sales.

In this report, I examine a model that predicts total sales of car seats as a function of all of the other variables in the data set. I then conduct diagnostic tests and attempt to improve the fit of the model.

# 1. Data

```{r}
library(ISLR)
library(ggplot2)
library(corrplot)
library(car)
```

Our data set contains 400 observations and 11 variables, with eight including our variable of interest `Sales` being numeric, two being binary factors indicating whether or not a location is in the US or urban, and one being a categorical variable with three levels describing shelving location. **Figure 1.1** provides more detail.

**Figure 1.2** is a correlation matrix of the eight numeric variables. Pairwise correlation is fairly high (0.58) between the price charged by the company and its competitors (`Price`, `CompPrice`), but it's not unusual for prices to correlate based on location. Nearly everything is more expensive in New York City than in Nebraska, and that includes car seats. Taking account of competitor pricing when modeling sales is important, even if it is correlated with our company's pricing.


# 2. Analyses

## Base regression analysis

The base model, regressing `Sales` on all of the available covariates, seems to be a decent fit. **Figure 2.1** contains a summary of the regression results.

7 variables, `CompPrice`, `Income`, `Advertising`, `Price`, `ShelveLoc`(Good and Medium), and `Age` are statistically significant at the 0.001 level, and all except for `Age` and `Price` have positive coefficients, indicating that car seat sales tend to be higher in locations with higher competitor prices, higher community incomes, higher local advertising budgets for the company, lower car seat prices, lower average age, and especially when the quality of the shelving locations for car seats isn't 'bad'. The intercept is also statistically significant, and its estimate of 5.66 implies that if all of the other variables were somehow 0, each location would sell 5,660 car seats on average.

`Population`, `Education`, and the `Urban` and `US` dummies are not statistically significant. Inspecting the 95% confidence intervals for the coefficient estimates in **figure 2.2** shows that they all contain 0, so they most likely have negligible effect on `Sales`.

Looking at magnitude, we see the highest effect on `Sales` from `ShelveLocGood` – on average, a ‘good’ shelving location is associated with a 4,850 unit increase in car seat sales compared to a ‘bad’ shelving location. Similarly, a ‘medium’ shelving location is associated with a 1,957 unit increase in sales compared to a ‘bad’ location. The other statistically significant variables have relatively small coefficient estimates. One-dollar increases in the competitor’s price `CompPrice`, the community’s income level `Income`, and the local advertising budget `Advertising` are associated with increases in sales of 93 units, 16 units, and 123 units respectively.

Our two statistically significant variables with a negative sign, `Age` and `Price`, tell us that a one-unit increase in the average age of the local population is associated with a 46 unit decrease in `Sales` and a one-dollar increase in `Price` is associated with a 95 unit decrease in `Sales`. These all make sense – if competitors goods are more expensive, if people are wealthier, if there’s money available for large ad campaigns, if there are a lot of children around (lower average age), and if car seat prices are lower, car seat sales should be higher.

The high F-statistic and its corresponding low p-value mean that our model provides a better fit than an intercept-only model. The R-squared tells us that the variance in our X variables explains about 87% of the variance in our target variable, but this can be marginally improved.

## Regression diagnostics

Next are some diagnostics beyond what is available from the regression summary. A scatterplot of our fitted values and studentized residuals can be found in **figure 3.1**. There doesn't appear to be any visual evidence of patterns suggesting heteroskedasticity or nonlinearities in the residuals. The residuals for observations 358, 357, and 298 are specifically marked as large in absolute value, which I deal with in the alternative specifications. 

The points along the diagonal line in the normal Q-Q plot in **figure 3.2** suggests that our residuals are approximately normal with mean 0. Again, the outliers from the previous figure show up as outliers in this plot. 

Using the `vif` function to check for multicollinearity yields **figure 3.3** where we can see that the highest VIF is around 2.1. VIFs of 4 to 5+ typically indicate multicollinearity, so there doesn't appear to be any problem here. Revisiting the correlation matrix also confirms that not many variables in our data have high pairwise correlation.

Conducting a Bonferroni outlier test in **figure 3.4** shows that observation 358 has the largest studentized residual. It also reports the next four observations with the largest studentized residuals, which I use in a later specification.

Finally, **figure 3.5** shows the results from a test for heteroskedasticity. The p-value of 0.77 means we fail to reject the null of homoskedasticity. This test validates what we saw visually in **figure 3.1**, constant variance of residuals for all fitted values.

Altogether it looks like OLS is completely valid, though there may be a few outliers affecting our results and the variables involved may interact with each other in non-additive ways.

## Alternate specifications

**Figure 4.1** contains a summary of the first alternate specification I tried. The only change made is the addition of a term interacting `Income` with `Advertising`. This new term's positive sign indicates that the effect of spending more money on advertising has a differing effect on car seat sales in different areas depending on the areas' average income. Interestingly, `Advertising` loses a little bit of significance - the interaction term probably accounts for some of the effect previously ascribed to one variable. This new interaction term is statistically significant at the 0.01 level so it appears to be useful for our model, but the adjusted R-squared only inches up from just below 0.87 to just above 0.87, so there may be other improvements to make.

For my second specification in **figure 4.2**, I exclude the five observations listed in the outlier test from **figure 3.4**. A few things change from the first specification - adjusted R-squared increases from 0.87 to 0.88 (and residual standard errors decrease), `Urban` becomes statistically significant at the 0.1 level, and strangely, `Population` switches signs (though it was never statistically significant to begin with). 

**Figure 4.3** holds the summary of my third and final specification, where I add another interaction term between `Income` and `Price`. Adjusted R-squared (and residual standard errors) increase (decrease) a tiny bit. This new term is statistically significant at the 0.05 level and its negative sign implies that in higher income areas, people are less likely to stop buying car seats just because of high prices, which makes sense - they can afford it. On the other hand, people in lower income areas are relatively more likely to avoid buying a car seat if it's expensive. In addition, `Education` is statistically significant at the 0.10 level and its sign tells us that areas with higher average levels of education tend to buy fewer car seats. 

# 3. Plots and Tables
```{r}
df = Carseats
str(df)
```
\begin{center}\textbf{Figure 1.1 - Structure of the Carseats data set}\end{center}
\hrulefill


```{r}
num = sapply(df, is.numeric)
corrplot(cor(df[,num]), method='number', type='upper', bg='black')
```
\begin{center}\textbf{Figure 1.2 - Correlation matrix}\end{center}
\hrulefill


```{r}
reg = lm(Sales ~ ., data=df) 
summary(reg)
```
\begin{center}\textbf{Figure 2.1 - Summary of base regression}\end{center}
\hrulefill


```{r}
round(confint(reg), 2)
```
\begin{center}\textbf{Figure 2.2 - 95\% confidence intervals for coefficient estimates, base model}\end{center}
\hrulefill


```{r}
plot(reg, which=1)
```
\begin{center}\textbf{Figure 3.1 - Fitted values vs. studentized residuals}\end{center}
\hrulefill


```{r}
plot(reg, which=2)
```
\begin{center}\textbf{Figure 3.2 - QQ plot}\end{center}
\hrulefill


```{r}
round(vif(reg), 2)
```
\begin{center}\textbf{Figure 3.3 - VIF results}\end{center}
\hrulefill


```{r}
outlierTest(reg, cutoff=Inf, n.max=5)
```
\begin{center}\textbf{Figure 3.4 - Outlier test results}\end{center}
\hrulefill


```{r}
ncvTest(reg)
```
\begin{center}\textbf{Figure 3.5 - Breusch-Pagan test results (heteroskedasticity)}\end{center}
\hrulefill


```{r}
reg0 = lm(Sales ~ . + Income:Advertising, data=df) 
summary(reg0)
```
\begin{center}\textbf{Figure 4.1 - First alternate specification}\end{center}
\hrulefill


```{r}
df1 = df[-c(358, 298, 357, 208, 16),] 

reg1 = lm(Sales ~ . + Income:Advertising, data=df1) 
summary(reg1)
```
\begin{center}\textbf{Figure 4.2 - Second alternate specification}\end{center}
\hrulefill

```{r}
reg2 = lm(Sales ~ . + Income:Advertising + Income:Price, data=df1) 
summary(reg2)
```
\begin{center}\textbf{Figure 4.3 - Third alternate specification}\end{center}
\hrulefill


# 4. Conclusions

In this report, I explained the regression output from the base model, then performed diagnostic tests to see if OLS was a valid approach. These tests included checking for nonlinearities, non-normality, and heteroskedasticity in the residuals, multicollinearity among the X variables, and outlier observations.

OLS appeared to be valid for the base model, but introducing sensible interaction terms and removing outliers improved the overall model fit, leading to more statistically significant coefficients and a higher adjusted R-squared. This new model excludes the most extreme observations and accounts for non-additive effects between income and advertising budget, as well as income and price, by interacting those variables.

The final model results weren't very different from the initial results. Many of the signs are the same and the magnitudes aren't very far off, but we attained a slightly better fit along with some additional insights about sales in areas with different levels of education and synergistic effects between income and other variables. 

# 5. Appendix

```{r ref.label=knitr::all_labels(), echo = T, eval = F}
```
