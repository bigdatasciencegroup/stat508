---
title: "STAT 508 Data Analysis Assignment 3"
author: "Sebastian Bautista"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment='>', echo=F)

library(Matrix)
library(psych)

generate.y = function(copies, X, b) { # given X and betas(?), generate a `copies` long vector of `y`
  lapply(1:copies, function(i) X%*%b + rnorm(nrow(X),0,1))
} #generate.y

beta.hat = function(sam=c(1:nrow(X)), A, X, y.vec) { # least-squares estimate of beta (vector), stored in a list
  b = vector("list", length(A)) # initialize list of same length as length of A
  for (i in 1:length(A)) { # using subset of X corresponding to columns in A[[i]] and observations in 'sam'
    b[[i]] = solve( t(X[sam, A[[i]]]) %*% X[sam, A[[i]]] ) %*% t(X[sam, A[[i]]]) %*% y.vec 
  }
  return(b)
} # beta.hat

get.ASPE = function(sam=c(1:nrow(X)), A, X, z, w) { # average squared prediction error
  PE = vector("list", length(z)) # initialize list of lists
  for (k in 1:length(z)) {
    PE[[k]] = vector("list", length(A))
  }
  for (i in 1:length(A)) {
    if (length(X[-sam, A[[i]]]) > 0) {
      for (k in 1:length(z)) {
        PE[[k]][[i]] = (1/length(z[[k]])) * sum( ( z[[k]] - X[-sam, A[[i]]] %*% w[[k]][[i]] )^2)
      }
    }
  }
  return(PE)
} # get.ASPE

optimal.models = function(A, X, y) {
  # Split the data using 1/3 for training and 2/3 for testing
  sam = sample(c(1:nrow(X)), round(nrow(X)/3, 0))
  ym = vector("list", length(y))
  yl = vector("list", length(y))
  for (i in 1:length(y)) {
    ym[[i]] = y[[i]][sam]
    yl[[i]] = y[[i]][-sam]
  }
 
  # Compute least-squares estimate using only the training set
  b = vector("list", length(ym))
  for (i in 1:length(ym)) {
    b[[i]] = beta.hat(sam, A, X, ym[[i]]) 
  }
  # Evaluating average squared prediction error using the testing part of the data
  ASPE = get.ASPE(sam, A, X, yl, b)
  # Pick out the model with the smallest average squared prediction error
  optimal.val = sapply(lapply(ASPE, function(x) sort(unlist(x))), function(x) x[1])
  optimal.mod = vector("character",length(optimal.val))
  for (i in 1:length(optimal.val)) {
    tmp = names(A[abs(unlist(ASPE[[i]]) - optimal.val[i]) < 0.0001]) # equal to smallest ASPE to within 0.0001 precision (can be modified)
    if (length(tmp) > 1) { # in case of (near) ties, break tie at random
      optimal.mod[i] = sample(tmp, 1)
    } else{
      optimal.mod[i] = tmp
    }
  }
  # tabulate the findings, including any models for which the frequency of selection is zero
  freq = vector("integer", length(A))
  names(freq) = names(A)
  freq[names(table(optimal.mod))] = table(optimal.mod)
 
  # return the frequency table as well as the subset of observations selected for the training set
  return(list(freq=freq, train=sort(sam))) 
} # optimal.models

######################
obs = 100
## Create collection of models
x1 = rep(1,obs)
x2 = 5*(1:obs) / obs
x3 = x2*sqrt(x2)
x4 = x2^2
x5 = log(x2)
x6 = x2*x5
X = cbind(x1,x2,x3,x4,x5,x6)
dimnames(X)[[1]] = c(1:obs)
rm(x1,x2,x3,x4,x5,x6,obs) # clean up: variables only needed to build the input matrix X
A = list(c(1,2,3,5,6), c(1,2,4,5,6), 
         c(1,2,5,6), c(1,3,5,6), c(1,4,5,6),c(2,3,5,6), c(2,4,5,6), 
         c(1,2,3), c(1,2,4), c(1,2,5), c(1,2,6), c(4,5,6),
         c(1,2), c(1,3), c(1,5), c(4,5), c(2,4))
names(A) = c("(1,2,3,5,6)", "(1,2,4,5,6)", 
             "(1,2,5,6)", "(1,3,5,6)", "(1,4,5,6)", "(2,3,5,6)", "(2,4,5,6)", 
             "(1,2,3)", "(1,2,4)", "(1,2,5)", "(1,2,6)", "(4,5,6)", 
             "(1,2)", "(1,3)", "(1,5)", "(4,5)", "(2,4)") 
get.Cp = function(A, X, z, w) {
  Cp = vector("list", length(z)) # initialize list of lists
  for (k in 1:length(z)) {
    Cp[[k]] = vector("list", length(A))
  }
 
  for (i in 1:length(A)) {
    for (k in 1:length(z)) {
      # get estimate of variance from largest model that (you hope) contains the true model
      sigma2 = summary(lm(z[[k]] ~ -1 + X[,1] + X[,2] + X[,3] + X[,4] + X[,5] + X[,6]))$sigma^2
      # compute Cp using formula (6.2) in ISLR Section 6.1.3
      Cp[[k]][[i]] = (1/nrow(X))*(sum((z[[k]] - X[, A[[i]]] %*% w[[k]][[i]])^2) + 2*length(A[[i]])*sigma2)
    }
  }
  return(Cp)
} # get.Cp

optimal.models.Cp = function(A, X, y) {
 
  # Compute least-squares estimates using full data
  b = vector("list", length(y))
  for (i in 1:length(y)) {
    b[[i]] = beta.hat(A=A, X=X, y.vec=y[[i]]) 
  }
 
  # Evaluate Mallows Cp
  Cp = get.Cp(A, X, y, b)
 
  # Pick out the model with the smallest value
  optimal.val = sapply(lapply(Cp, function(x) sort(unlist(x))), function(x) x[1])
  optimal.mod = vector("character",length(optimal.val))
  for (i in 1:length(optimal.val)) {
    tmp = names(A[abs(unlist(Cp[[i]]) - optimal.val[i]) < 0.0001]) # equal to smallest Cp to within 0.0001 precision (can be modified)
    if (length(tmp) > 1) { # in case of (near) ties, break tie at random
      optimal.mod[i] = sample(tmp, 1)
    } else{
      optimal.mod[i] = tmp
    }
  }
 
  # tabulate the findings, including any models for which the frequency of selection is zero
  freq = vector("integer", length(A))
  names(freq) = names(A)
  freq[names(table(optimal.mod))] = table(optimal.mod)
 
  # return the frequency table as well as the subset of observations selected for the training set
  return(list(freq=freq)) 
} # optimal.models.Cp

# Part 3 setup - changing definition of `x2`, creating new X0
obs = 100
## Create collection of models
x1 = rep(1,obs)
x2 = 1*(1:obs) / obs
x3 = x2*sqrt(x2)
x4 = x2^2
x5 = log(x2)
x6 = x2*x5
X0 = cbind(x1,x2,x3,x4,x5,x6)
dimnames(X0)[[1]] = c(1:obs)
rm(x1,x2,x3,x4,x5,x6,obs) # clean up: variables only needed to build the input matrix X
```

# 0. Introduction

This week, we explore Monte Carlo cross-validation for model selection. We generate our own dataset and see how distinct but similar implementations of learners can differ in their performance, varying train-test splits, our response variable, or our criteria for a 'good fit'. We also investigate the effects of seemingly subtle changes in our data.

(((SUMMARY OF MAIN FINDINGS)))

# 1. Data

We generate our own data here based on Professor Talih's R code. Two data sets are generated - one labeled `X` here, containing the data for parts I and II, and one labeled `X0`, containing the modified data for part III. Both contain 6 variables, defined in the assignment text.

# 2. Analyses

## Part I

*Q: Summarize the findings from these two simulations. How likely is the procedure to correctly learn the "true" model? Which simulation/implementation of the learner performs better? Why do you think that is? Explain.*

**Figure 1.1** is a chart showing the frequency that each of the models in `A` was selected as optimal in the first simulation, which involves a fixed split and 1000 different instances of y. In simulation 1, the true model `(1,2,4)` is selected 39% of the time, or 390 times out of 1000 runs. Coming up in second place, `(2,4,5,6)` is selected 17% of the time, less than half of the frequency that the true model is selected. The rest of the modles are selected 0% to 11% of the time, so this implementation involving varying the response variable and keeping the train-test split fixed looks to be fairly good at learning the true model.

**Figure 1.2** is the same frequency chart as **1.1**, except using a single instance of y and 1000 different splits. In this second simulation, the true model is selected 32% of the time. Compared to the previous result of 39%, this implementation of the learner is slightly less likely to correctly learn the true model, though both succeed in picking out `(1,2,4)`. 

Interestingly, the rank-ordering of the false models changes completely. In second place with 20% is `(2,3,5,6)`, which came in third in the first simulation. The model that was ranked second in the first simulation, `(2,4,5,6)`, is chosen less than 1% of the time under this implementation. Tied in third place are `(1,2,3,5,6)` and `(1,4,5,6)`, which are both selected at over half of the frequency as the true model. Compared to the previous simulation, a lower number of incorrect models are selected (more 0s), but certain incorrect models are chosen more frequently. Because of this, the first implementation of the learner performs better.

## Part II

*Q: Compare the performance of the Monte Carlo learners in simulations 1 and 2 to the performance of Mallows' Cp in model selection.*

**Figure 2** shows the model selection frequency chart using Mallows' Cp as the criterion and using all of the rows in X. Since the true model `(1,2,4)` is chosen 68% of the time, it looks like Cp does very well in this case. Contrast this with the second place model, `(2,4,5,6)`, which is chosen only 11% of the time - less than six times the frequency as the true model.

(((SOMETHING ABOUT THE DIFFERENCE BETWEEN Cp AND ASPE AS METRICS)))

## Part III

*Q: Rerun the previous three learners after redefining `x2` and examine the results comparatively. How likely are these procedures to correctly learn the "true" model in this case? Why do you think it is harder to learn the true model for values of $x_i \in [0,1]$ instead of $x_i \in [0,5]$?*

**Figures 3.1, 3.2, and 3.3** show the results for simulation 1, simulation 2, and the Mallows' Cp implementation after changing the definition of `x2`. 

**Figure 3.4** shows summary statistics for the first dataset `X` alongside the statistics for the modified dataset `X0`.

(((A LOT MORE STUFF HERE, INTERPRETATION ETC.)))

# 3. Plots and Tables

```{r}
## Part I Simulation 1: Distribution of optimal models for fixed split over different instances of y
copies = 1000
set.seed(17)
ylist = generate.y(copies, X[, A[[9]]], rep(2,3))
set.seed(11)
seed.optimal = optimal.models(A, X, ylist)
plot(seed.optimal$freq, type="h", main=paste("Proportion of times each model is selected as the optimal model\n for an arbitrary but fixed split over", copies, "instances of y"), sub=paste('Part I, Simulation 1'), xlab="", ylab="", bty="n", axes=F)
axis(1, at=1:length(A), labels=names(A), las=2)
points(1:length(A), seed.optimal$freq, pch=20)
text(x=1:length(A), y=seed.optimal$freq+copies/100, labels=round(seed.optimal$freq/copies, 2), cex=0.7)
```
\begin{center}\textbf{Figure 1.1}\end{center}

\hrulefill

```{r}
## Part I Simulation 2: Distribution of optimal models for single instance of y over multiple splits
splits = 1000
set.seed(17)
ylist = generate.y(1, X[, A[[9]]], rep(2,3))
set.seed(11)
iter.optimal = vector("list", splits)
for (iter in 1:splits) {
 iter.optimal[[iter]] = optimal.models(A, X, ylist)
}
## Sum across iterations to get frequency of selection of each model
iter.freq = iter.optimal[[1]]$freq
for (iter in 2:splits) {
 iter.freq = iter.freq + iter.optimal[[iter]]$freq
}
plot(iter.freq, type="h", main=paste("Proportion of times each model is selected as the optimal model\n for a single instance of y over", splits, "splits"), sub=paste('Part I, Simulation 2'), xlab="", ylab="", bty="n", axes=F)
axis(1, at=1:length(A), labels=names(A), las=2)
points(1:length(A), iter.freq, pch=20)
text(x=1:length(A), y=iter.freq+splits/100, labels=round(iter.freq/splits, 2), cex=0.7)
```
\begin{center}\textbf{Figure 1.2}\end{center}

\hrulefill

```{r}
## Part II, Mallows' Cp
copies = 1000

set.seed(17)
ylist = generate.y(copies, X[, A[[9]]], rep(2,3))
set.seed(11)
Cp.optimal = optimal.models.Cp(A, X, ylist)
plot(Cp.optimal$freq, type="h", main=paste("Proportion of times each model is selected as the optimal model\n for an arbitrary but fixed split over", copies, "instances of y"), sub=paste("Part II, Mallows' Cp"), xlab="", ylab="", bty="n", axes=F)
axis(1, at=1:length(A), labels=names(A), las=2)
points(1:length(A), Cp.optimal$freq, pch=20)
text(x=1:length(A), y=Cp.optimal$freq+copies/100, labels=round(Cp.optimal$freq/copies, 2), cex=0.7)
```
\begin{center}\textbf{Figure 2}\end{center}

\hrulefill

```{r}
## Part III, Simulation 1
copies = 1000
set.seed(17)
ylist = generate.y(copies, X0[, A[[9]]], rep(2,3))
set.seed(11)
seed.optimal = optimal.models(A, X0, ylist)
plot(seed.optimal$freq, type="h", main=paste("Proportion of times each model is selected as the optimal model\n for an arbitrary but fixed split over", copies, "instances of y"), sub=paste('Part III, Simulation 1'), xlab="", ylab="", bty="n", axes=F)
axis(1, at=1:length(A), labels=names(A), las=2)
points(1:length(A), seed.optimal$freq, pch=20)
text(x=1:length(A), y=seed.optimal$freq+copies/100, labels=round(seed.optimal$freq/copies, 2), cex=0.7)
```
\begin{center}\textbf{Figure 3.1}\end{center}

\hrulefill

```{r}
## Part III, Simulation 2
splits = 1000
set.seed(17)
ylist = generate.y(1, X0[, A[[9]]], rep(2,3))
set.seed(11)
iter.optimal = vector("list", splits)
for (iter in 1:splits) {
 iter.optimal[[iter]] = optimal.models(A, X0, ylist)
}
## Sum across iterations to get frequency of selection of each model
iter.freq = iter.optimal[[1]]$freq
for (iter in 2:splits) {
 iter.freq = iter.freq + iter.optimal[[iter]]$freq
}
plot(iter.freq, type="h", main=paste("Proportion of times each model is selected as the optimal model\n for a single instance of y over", splits, "splits"), sub=paste('Part III, Simulation 2'), xlab="", ylab="", bty="n", axes=F)
axis(1, at=1:length(A), labels=names(A), las=2)
points(1:length(A), iter.freq, pch=20)
text(x=1:length(A), y=iter.freq+splits/100, labels=round(iter.freq/splits, 2), cex=0.7)
```
\begin{center}\textbf{Figure 3.2}\end{center}

\hrulefill

```{r}
## Part III, Mallows' Cp
copies = 1000
set.seed(17)
ylist = generate.y(copies, X0[, A[[9]]], rep(2,3))
set.seed(11)
Cp.optimal = optimal.models.Cp(A, X0, ylist)
plot(Cp.optimal$freq, type="h", main=paste("Proportion of times each model is selected as the optimal model\n for an arbitrary but fixed split over", copies, "instances of y"), sub=paste("Part III, Mallows' Cp"), xlab="", ylab="", bty="n", axes=F)
axis(1, at=1:length(A), labels=names(A), las=2)
points(1:length(A), Cp.optimal$freq, pch=20)
text(x=1:length(A), y=Cp.optimal$freq+copies/100, labels=round(Cp.optimal$freq/copies, 2), cex=0.7)
```
\begin{center}\textbf{Figure 3.3}\end{center}

\hrulefill

```{r}
print('Description of X')
describe(X, skew=F, check=F, fast=T)
print('Description of X0 (x2 redefined for part III)')
describe(X0, skew=F, check=F, fast=T)
```
\begin{center}\textbf{Figure 3.4 - Comparison of X and X0}\end{center}

Note the changes in standard deviation in x2, x3, and x4. After x2 is redefined, all three variables have nearly identical standard deviation and range, and similar means. This coupled with the learners choosing the model (1,3) suggests that, under the new definitions, the three variables have very similar contributions when it comes to predicting our target variable. This is reinforced by the fact that the second place model is (1,2).

\hrulefill

# 4. Conclusions

# 5. Appendix

```{r ref.label=knitr::all_labels(), echo = T, eval = F}
```