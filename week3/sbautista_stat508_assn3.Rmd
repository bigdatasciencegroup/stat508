---
title: "STAT 508 Data Analysis Assignment 3"
author: "Sebastian Bautista"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment='>', echo=F)

library(Matrix)
library(psych)

generate.y = function(copies, X, b) { # given X and betas(?), generate a `copies` long vector of `y`
  lapply(1:copies, function(i) X%*%b + rnorm(nrow(X),0,1))
} #generate.y

beta.hat = function(sam=c(1:nrow(X)), A, X, y.vec) { # least-squares estimate of beta (vector), stored in a list
  b = vector("list", length(A)) # initialize list of same length as length of A
  for (i in 1:length(A)) { # using subset of X corresponding to columns in A[[i]] and observations in 'sam'
    b[[i]] = solve( t(X[sam, A[[i]]]) %*% X[sam, A[[i]]] ) %*% t(X[sam, A[[i]]]) %*% y.vec 
  }
  return(b)
} # beta.hat

get.ASPE = function(sam=c(1:nrow(X)), A, X, z, w) { # average squared prediction error
  PE = vector("list", length(z)) # initialize list of lists
  for (k in 1:length(z)) {
    PE[[k]] = vector("list", length(A))
  }
  for (i in 1:length(A)) {
    if (length(X[-sam, A[[i]]]) > 0) {
      for (k in 1:length(z)) {
        PE[[k]][[i]] = (1/length(z[[k]])) * sum( ( z[[k]] - X[-sam, A[[i]]] %*% w[[k]][[i]] )^2)
      }
    }
  }
  return(PE)
} # get.ASPE

optimal.models = function(A, X, y) {
  # Split the data using 1/3 for training and 2/3 for testing
  sam = sample(c(1:nrow(X)), round(nrow(X)/3, 0))
  ym = vector("list", length(y))
  yl = vector("list", length(y))
  for (i in 1:length(y)) {
    ym[[i]] = y[[i]][sam]
    yl[[i]] = y[[i]][-sam]
  }
 
  # Compute least-squares estimate using only the training set
  b = vector("list", length(ym))
  for (i in 1:length(ym)) {
    b[[i]] = beta.hat(sam, A, X, ym[[i]]) 
  }
  # Evaluating average squared prediction error using the testing part of the data
  ASPE = get.ASPE(sam, A, X, yl, b)
  # Pick out the model with the smallest average squared prediction error
  optimal.val = sapply(lapply(ASPE, function(x) sort(unlist(x))), function(x) x[1])
  optimal.mod = vector("character",length(optimal.val))
  for (i in 1:length(optimal.val)) {
    tmp = names(A[abs(unlist(ASPE[[i]]) - optimal.val[i]) < 0.0001]) # equal to smallest ASPE to within 0.0001 precision (can be modified)
    if (length(tmp) > 1) { # in case of (near) ties, break tie at random
      optimal.mod[i] = sample(tmp, 1)
    } else{
      optimal.mod[i] = tmp
    }
  }
  # tabulate the findings, including any models for which the frequency of selection is zero
  freq = vector("integer", length(A))
  names(freq) = names(A)
  freq[names(table(optimal.mod))] = table(optimal.mod)
 
  # return the frequency table as well as the subset of observations selected for the training set
  return(list(freq=freq, train=sort(sam))) 
} # optimal.models

######################
obs = 100
## Create collection of models
x1 = rep(1,obs)
x2 = 5*(1:obs) / obs
x3 = x2*sqrt(x2)
x4 = x2^2
x5 = log(x2)
x6 = x2*x5
X = cbind(x1,x2,x3,x4,x5,x6)
dimnames(X)[[1]] = c(1:obs)
rm(x1,x2,x3,x4,x5,x6,obs) # clean up: variables only needed to build the input matrix X
A = list(c(1,2,3,5,6), c(1,2,4,5,6), 
         c(1,2,5,6), c(1,3,5,6), c(1,4,5,6),c(2,3,5,6), c(2,4,5,6), 
         c(1,2,3), c(1,2,4), c(1,2,5), c(1,2,6), c(4,5,6),
         c(1,2), c(1,3), c(1,5), c(4,5), c(2,4))
names(A) = c("(1,2,3,5,6)", "(1,2,4,5,6)", 
             "(1,2,5,6)", "(1,3,5,6)", "(1,4,5,6)", "(2,3,5,6)", "(2,4,5,6)", 
             "(1,2,3)", "(1,2,4)", "(1,2,5)", "(1,2,6)", "(4,5,6)", 
             "(1,2)", "(1,3)", "(1,5)", "(4,5)", "(2,4)") 
get.Cp = function(A, X, z, w) {
  Cp = vector("list", length(z)) # initialize list of lists
  for (k in 1:length(z)) {
    Cp[[k]] = vector("list", length(A))
  }
 
  for (i in 1:length(A)) {
    for (k in 1:length(z)) {
      # get estimate of variance from largest model that (you hope) contains the true model
      sigma2 = summary(lm(z[[k]] ~ -1 + X[,1] + X[,2] + X[,3] + X[,4] + X[,5] + X[,6]))$sigma^2
      # compute Cp using formula (6.2) in ISLR Section 6.1.3
      Cp[[k]][[i]] = (1/nrow(X))*(sum((z[[k]] - X[, A[[i]]] %*% w[[k]][[i]])^2) + 2*length(A[[i]])*sigma2)
    }
  }
  return(Cp)
} # get.Cp

optimal.models.Cp = function(A, X, y) {
 
  # Compute least-squares estimates using full data
  b = vector("list", length(y))
  for (i in 1:length(y)) {
    b[[i]] = beta.hat(A=A, X=X, y.vec=y[[i]]) 
  }
 
  # Evaluate Mallows Cp
  Cp = get.Cp(A, X, y, b)
 
  # Pick out the model with the smallest value
  optimal.val = sapply(lapply(Cp, function(x) sort(unlist(x))), function(x) x[1])
  optimal.mod = vector("character",length(optimal.val))
  for (i in 1:length(optimal.val)) {
    tmp = names(A[abs(unlist(Cp[[i]]) - optimal.val[i]) < 0.0001]) # equal to smallest Cp to within 0.0001 precision (can be modified)
    if (length(tmp) > 1) { # in case of (near) ties, break tie at random
      optimal.mod[i] = sample(tmp, 1)
    } else{
      optimal.mod[i] = tmp
    }
  }
 
  # tabulate the findings, including any models for which the frequency of selection is zero
  freq = vector("integer", length(A))
  names(freq) = names(A)
  freq[names(table(optimal.mod))] = table(optimal.mod)
 
  # return the frequency table as well as the subset of observations selected for the training set
  return(list(freq=freq)) 
} # optimal.models.Cp

# Part 3 setup - changing definition of `x2`, creating new X0
obs = 100
## Create collection of models
x1 = rep(1,obs)
x2 = 1*(1:obs) / obs
x3 = x2*sqrt(x2)
x4 = x2^2
x5 = log(x2)
x6 = x2*x5
X0 = cbind(x1,x2,x3,x4,x5,x6)
dimnames(X0)[[1]] = c(1:obs)
rm(x1,x2,x3,x4,x5,x6,obs) # clean up: variables only needed to build the input matrix X
```

# 0. Introduction

This week, we explore Monte Carlo cross-validation for model selection. We generate our own dataset and see how distinct but similar implementations of learners can differ in their performance, varying train-test splits, our response variable, or our criteria for a 'good fit'. We also investigate the effects of subtle changes in our data.

In part I, I compare a fixed validation set approach and a varying split approach, finding that bias and variance change between the simulations. In part II, I see how training error can be adjusted for model size using Mallows' *Cp*. In part III, I find that results can be muddled when variables start to look more homogeneous.

# 1. Data

We generate our own data here using Professor Talih's R code. I create two data sets - one named `X`, containing the data for parts I and II, and one named `X0`, containing the modified data after redefining variable `x2` for part III. Both contain 6 variables, defined in the assignment text. The true model includes `x1`, `x2`, `x4`, and a random noise variable. 

# 2. Analyses

## Part I

*Q: Summarize the findings from these two simulations. How likely is the procedure to correctly learn the "true" model? Which simulation/implementation of the learner performs better? Why do you think that is? Explain.*

**Figure 1.1** shows the frequency that each of the models in `A` was selected as optimal in the first simulation, which involves a fixed split (validation set approach) and 1000 different instances of y. In simulation 1, the true model `(1,2,4)` is selected 39% of the time, or 390 times out of 1000 runs. Coming up in second place, `(2,4,5,6)` is selected 17% of the time, less than half of the frequency that the true model is selected. The rest of the models are selected 0% to 11% of the time, so this implementation involving varying the response variable and keeping the train-test split fixed looks to be fairly good at learning the true model. However, this validation set approach can have varying results depending on the split, which depends on the seed. 

**Figure 1.2** is the same frequency chart as in **1.1**, except using a single instance of y and 1000 different splits. In this second simulation, the true model is selected 32% of the time. Compared to the previous result of 39%, this implementation of the learner is slightly less likely to correctly learn the true model, though both succeed in picking out `(1,2,4)`. This approach has higher variance but lower bias, since the training and testing data changes every time.

Interestingly, the rank-ordering of the incorrect models is different from the first simulation. In second place with 20% is `(2,3,5,6)`, which came in third earlier. The model that was ranked second in the first simulation, `(2,4,5,6)`, is chosen less than 1% of the time under this implementation. Tied in third place are `(1,2,3,5,6)` and `(1,4,5,6)`, which are both selected at over half of the frequency as the true model. Compared to the previous simulation, a lower number of incorrect models are selected (more 0s), but certain incorrect models are chosen more frequently. This reflects the differences in bias and variance between the validation set approach and the varying split approach.

## Part II

*Q: Compare the performance of the Monte Carlo learners in simulations 1 and 2 to the performance of Mallows' Cp in model selection.*

**Figure 2** shows the model selection frequency chart using Mallows' *Cp* as the criterion and using all of the rows in X. Since the true model `(1,2,4)` is chosen 68% of the time, it looks like *Cp* does very well in this case. Contrast this with the second place model, `(2,4,5,6)`, which is chosen only 11% of the time - less than six times the frequency as the true model. Compared to the first two simulations, the true model is much more likely to be learned, and alternate models are much less likely to be considered.

The *Cp* statistic penalizes the training RSS because the training error tends to underestimate the test error. The formula for the statistic is given by $C_p=\frac{1}{n}(RSS + 2d\hat{\sigma}^2)$ where the last term penalizes models with relatively many predictors. Because of this, some of the four-variable models that were chosen up to 20% of the time in the first two simulations are chosen much less of the time here, with the true three-variable model being the clear winner.

## Part III

*Q: Rerun the previous three learners after redefining `x2` and examine the results comparatively. How likely are these procedures to correctly learn the "true" model in this case? Why do you think it is harder to learn the true model for values of $x_i \in [0,1]$ instead of $x_i \in [0,5]$?*

**Figures 3.1, 3.2, and 3.3** show the results for simulation 1, simulation 2, and the Mallows' *Cp* implementation after changing the definition of `x2`. The results look poor - `(1,3)`, `(1,2)`, and `(1,4,5,6)` are chosen over the true model in all three of the implementations. It looks like `x3` has become a lot more important for some reason.

**Figure 3.4** shows summary statistics for the first data set `X` alongside the statistics for the modified data set `X0`. Looking at `x2`, `x3`, and `x4` in particular, it's clear that redefining `x2` changed the relationship between these three variables. Their ranges and standard deviations are all quite different from each other in the original data set, but after the transformation, they all have a range of about [0,1] and standard deviation of 0.3.
 
This coupled with the learners choosing the model `(1,3)` suggests that, after changing `x2`, the three variables have very similar contributions to the model. Even though the true model is `(1,2,4)`, `(1,3)` is now basically close enough that it is chosen most of the time, especially when using *Cp* (**figure 3.3**), which penalizes more complex models.

# 3. Plots and Tables

```{r}
## Part I Simulation 1: Distribution of optimal models for fixed split over different instances of y
copies = 1000
set.seed(17)
ylist = generate.y(copies, X[, A[[9]]], rep(2,3))
set.seed(11)
seed.optimal = optimal.models(A, X, ylist)
plot(seed.optimal$freq, type="h", main=paste("Proportion of times each model is selected as the optimal model\n for an arbitrary but fixed split over", copies, "instances of y"), sub=paste('Part I, Simulation 1'), xlab="", ylab="", bty="n", axes=F)
axis(1, at=1:length(A), labels=names(A), las=2)
points(1:length(A), seed.optimal$freq, pch=20)
text(x=1:length(A), y=seed.optimal$freq+copies/100, labels=round(seed.optimal$freq/copies, 2), cex=0.7)
```
\begin{center}\textbf{Figure 1.1}\end{center}

\hrulefill

```{r}
## Part I Simulation 2: Distribution of optimal models for single instance of y over multiple splits
splits = 1000
set.seed(17)
ylist = generate.y(1, X[, A[[9]]], rep(2,3))
set.seed(11)
iter.optimal = vector("list", splits)
for (iter in 1:splits) {
 iter.optimal[[iter]] = optimal.models(A, X, ylist)
}
## Sum across iterations to get frequency of selection of each model
iter.freq = iter.optimal[[1]]$freq
for (iter in 2:splits) {
 iter.freq = iter.freq + iter.optimal[[iter]]$freq
}
plot(iter.freq, type="h", main=paste("Proportion of times each model is selected as the optimal model\n for a single instance of y over", splits, "splits"), sub=paste('Part I, Simulation 2'), xlab="", ylab="", bty="n", axes=F)
axis(1, at=1:length(A), labels=names(A), las=2)
points(1:length(A), iter.freq, pch=20)
text(x=1:length(A), y=iter.freq+splits/100, labels=round(iter.freq/splits, 2), cex=0.7)
```
\begin{center}\textbf{Figure 1.2}\end{center}

\hrulefill

```{r}
## Part II, Mallows' Cp
copies = 1000

set.seed(17)
ylist = generate.y(copies, X[, A[[9]]], rep(2,3))
set.seed(11)
Cp.optimal = optimal.models.Cp(A, X, ylist)
plot(Cp.optimal$freq, type="h", main=paste("Proportion of times each model is selected as the optimal model\n for an arbitrary but fixed split over", copies, "instances of y"), sub=paste("Part II, Mallows' Cp"), xlab="", ylab="", bty="n", axes=F)
axis(1, at=1:length(A), labels=names(A), las=2)
points(1:length(A), Cp.optimal$freq, pch=20)
text(x=1:length(A), y=Cp.optimal$freq+copies/100, labels=round(Cp.optimal$freq/copies, 2), cex=0.7)
```
\begin{center}\textbf{Figure 2}\end{center}

\hrulefill

```{r}
## Part III, Simulation 1
copies = 1000
set.seed(17)
ylist = generate.y(copies, X0[, A[[9]]], rep(2,3))
set.seed(11)
seed.optimal = optimal.models(A, X0, ylist)
plot(seed.optimal$freq, type="h", main=paste("Proportion of times each model is selected as the optimal model\n for an arbitrary but fixed split over", copies, "instances of y"), sub=paste('Part III, Simulation 1'), xlab="", ylab="", bty="n", axes=F)
axis(1, at=1:length(A), labels=names(A), las=2)
points(1:length(A), seed.optimal$freq, pch=20)
text(x=1:length(A), y=seed.optimal$freq+copies/100, labels=round(seed.optimal$freq/copies, 2), cex=0.7)
```
\begin{center}\textbf{Figure 3.1}\end{center}

\hrulefill

```{r}
## Part III, Simulation 2
splits = 1000
set.seed(17)
ylist = generate.y(1, X0[, A[[9]]], rep(2,3))
set.seed(11)
iter.optimal = vector("list", splits)
for (iter in 1:splits) {
 iter.optimal[[iter]] = optimal.models(A, X0, ylist)
}
## Sum across iterations to get frequency of selection of each model
iter.freq = iter.optimal[[1]]$freq
for (iter in 2:splits) {
 iter.freq = iter.freq + iter.optimal[[iter]]$freq
}
plot(iter.freq, type="h", main=paste("Proportion of times each model is selected as the optimal model\n for a single instance of y over", splits, "splits"), sub=paste('Part III, Simulation 2'), xlab="", ylab="", bty="n", axes=F)
axis(1, at=1:length(A), labels=names(A), las=2)
points(1:length(A), iter.freq, pch=20)
text(x=1:length(A), y=iter.freq+splits/100, labels=round(iter.freq/splits, 2), cex=0.7)
```
\begin{center}\textbf{Figure 3.2}\end{center}

\hrulefill

```{r}
## Part III, Mallows' Cp
copies = 1000
set.seed(17)
ylist = generate.y(copies, X0[, A[[9]]], rep(2,3))
set.seed(11)
Cp.optimal = optimal.models.Cp(A, X0, ylist)
plot(Cp.optimal$freq, type="h", main=paste("Proportion of times each model is selected as the optimal model\n for an arbitrary but fixed split over", copies, "instances of y"), sub=paste("Part III, Mallows' Cp"), xlab="", ylab="", bty="n", axes=F)
axis(1, at=1:length(A), labels=names(A), las=2)
points(1:length(A), Cp.optimal$freq, pch=20)
text(x=1:length(A), y=Cp.optimal$freq+copies/100, labels=round(Cp.optimal$freq/copies, 2), cex=0.7)
```
\begin{center}\textbf{Figure 3.3}\end{center}

\hrulefill

```{r}
print('Description of X')
describe(X, skew=F, check=F, fast=T)
print('Description of X0 (x2 redefined for part III)')
describe(X0, skew=F, check=F, fast=T)
```
\begin{center}\textbf{Figure 3.4 - Comparison of X and X0}\end{center}

\hrulefill

# 4. Conclusions

In this assignment, we explored different "species" of cross-validation and saw how results may vary. In part I, I compared the high-bias low-variance validation set technique with the low-bias high-variance varying split technique. In part II, I found that penalizing larger models by using Mallows' *Cp* led to even more confident results than in part I. In part III, I discovered that model selection becomes more complicated as variables start to look more alike.

# 5. Appendix

```{r ref.label=knitr::all_labels(), echo = T, eval = F}
```