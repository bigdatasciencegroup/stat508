---
title: "STAT 508 Data Analysis Assignment 8"
author: "Sebastian Bautista"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment='>', echo=F)

library(readxl)
eretail = read_excel("Online Retail.xlsx")
dim(eretail)
names(eretail)

eretail = eretail[eretail$Country != "Unspecified",] # remove 'unspecified' country
eretail = eretail[eretail$Quantity > 0,]             # remove returns/cancellations

IDtab = table(eretail$Country, eretail$CustomerID)   # crosstab country by customer ID
IDtab = apply(IDtab >0, 2, sum)                      # is any customer ID duplicated across countries?
duplicateIDs = names(IDtab[IDtab > 1])               # duplicate IDs to clean up
eretail = eretail[!is.element(eretail$CustomerID, duplicateIDs),]
rm(IDtab)

eretail$InvoiceMth = substr(eretail$InvoiceDate, 1, 7)         # extract month of invoice
eretail = eretail[as.vector(eretail$InvoiceMth) != "2011-12",] # remove December 2011 as it only covers first week

eretail$Amount = eretail$Quantity * eretail$UnitPrice           # compute amount per invoice item

eaggr = aggregate(Amount~Country+CustomerID, data=eretail, sum) # compute aggregate amount spent per customer
row.names(eaggr) = eaggr$CustomerID
eaggr = eaggr[,-2]
eaggr = cbind(eaggr, aggregate(InvoiceMth~CustomerID, data=eretail, min)[,-1]) # 1st month of customer interaction
names(eaggr)[3] = "FirstMth"
eaggr = cbind(eaggr, aggregate(InvoiceMth~CustomerID, data=eretail, max)[,-1]) # last month of cust. interaction
names(eaggr)[4] = "LastMth"

# relabel months and compute duration of customer interaction
levels(eaggr$FirstMth) = 1:12
levels(eaggr$LastMth) = 1:12
eaggr$FirstMth = as.numeric(as.vector(eaggr$FirstMth))
eaggr$LastMth = as.numeric(as.vector(eaggr$LastMth))
eaggr$Months = eaggr$LastMth - eaggr$FirstMth + 1

eaggr = cbind(eaggr, apply( table(eretail$CustomerID, eretail$InvoiceMth) , 1, sum ) )
names(eaggr)[6] = "Purchases"

# Some useful statistics (which you may or may not decide to use)
eaggr$Amount.per.Purchase = eaggr$Amount / eaggr$Purchases
eaggr$Purchases.per.Month = eaggr$Purchases / eaggr$Months
eaggr$Amount.per.Month = eaggr$Amount / eaggr$Months

eaggr[1:30,]

df = eaggr
colnames(df) = tolower(colnames(df))
```

# 0. Introduction

This week, we're using clustering techniques on sales data from a UK-based company. The original data is at the transaction level, but we aggregate to the customer level and create a few more variables in order to do our analysis. I begin by changing the data into a form that clustering algorithms can work with. Then, I try out k-means clustering and hierarchical clustering, varying values of *k* for both algorithms and methods of calculating distance for `hclust`. I find that k-means results in much more interesting and informative clusters compared to hierarchical clustering, regardless of whether complete, average, single, or centroid linkage is used for `hclust`. 

# 1. Data

The data set created after aggregation, `eaggr`, contains 4286 observations and 9 variables. Each observation represents a customer and each row is indexed by customer ID. Variables include the customer's country, the total amount of pounds spent (quantity times price), the first and last months of customer interaction, and the number of purchases by a customer, as well as the amount of pounds spent per purchase, the number of purchases per month, and the amount of pounds spent per month. All but `country` are numeric; in **figure 1.1** I use this character variable to create a dummy variable called `intl` which is equal to 1 if the customer is outside of the UK and 0 otherwise. **Figure 1.2** contains summary statistics for the final input data set.

# 2. Analyses

**Figures 2.1** through **2.4** show the results of applying k-means clustering with *k* ranging from 2 to 5. I report the size of the clusters both in terms of number of observations and percentage of the whole data set. Cluster means are also shown in order to numerically compare the clusters. I divide the between sum of squares by the total sum of squares to represent the variance in the data set explained by the clustering scheme. I stop at k=5 because of the singleton cluster and because the clusters chosen seem to be appropriate - by assigning the observations to these 5 clusters rather than `nobs` clusters, we achieve a reduction in sum of squares of about 59%. 

**Figure 2.4** shows the results for k=5. Looking at the cluster means, the first cluster makes up 40% of the data and is made up of customers with a relatively long relationship with the company (`months` is positive) and above average `purchases`, but tend to spend an average amount of pounds on those purchases compared to our other clusters (`amount` and `amount.per.purchase` close to 0). The members of this cluster also are mostly, but not all, from the UK.

The second cluster is notable in that it is made up of one observation, the first customer in the data. This customer is the positive outlier for `amount.per.purchase` and `amount.per.month`, making relatively infrequent but large purchases (lowest mean `purchases.per.month`). Surprisingly, this customer-cluster also has the shortest relationship with the vendor (lowest `months`). Its value for `lastmth` also signals that it hasn't made an order recently, which might be something worth looking into. It is unsurprising that this customer is in its own singleton cluster, since its behavior is much different from all of the others. In practice, this customer would be one to focus on - they haven't ordered anything in a while and their business is worth a lot.

The third cluster makes up about a quarter of our sample and is identifiable by having the lowest mean `amount`, implying that these customers are small spenders. These customers also have relatively low mean `months`, so they tend to be newer customers, and `intl` is close to 0 so there is a good mix of UK and international customers in this cluster. The rest of the variables have means close to 0 so this may be interpreted as an "average" cluster.

Cluster four has the highest means for `amount` and `purchases.per.month`, so these 13 customers spend a lot of money across many medium-to-large sized purchases (second highest mean `amount.per.purchase` but nothing even close to our second cluster). This group has the most international customers (high `intl`) so these are most likely wholesalers. They also have the highest mean for `months`, so these are the oldest and longest customers. These 13 are a very important subset of the customer base; similarly to the second cluster, they make up a disproportionately large amount of business.

Finally, the fifth cluster makes up a little over a third of the sample and is characterized by being more recent customers (high `firstmth` and `lastmth`) and slightly higher than average `purchases.per.month`, but the rest of the means aren't too different from the rest of the sample. These customers also tend to be more `intl` compared to all but the fourth cluster. 

The main takeaway that I gathered from this exercise is that those 14 customers in the second and fourth clusters are worth paying extra attention to. They dwarf the rest of the customers in their spending amounts, frequency, and loyalty, and should be incentivized to keep doing business with the company. 

The results I uncovered using hierarchical clustering weren't nearly as neat. **Figures 3.1** through **3.8** show results using `hclust` varying linkage methods and values of `k` between 3 and 10. For the sake of space, I go through the results at a high level here. For the most part, none of the linkage methods are successful in meaningful separation - at all levels of `k` there are a lot of clusters of size 1-5 with a dominating 4200+ strong cluster in every case. A chart visualizing k=5 using complete linkage is in **figure 3.9**. All in all, hierarchical clustering doesn't look like it's appropriate for this data set.

# 3. Plots and Tables

```{r}
cat('The data contains', length(unique(df$country)), 'unique countries\n')
table(df$country)

# create `intl` dummy variable because of predominance of UK customers
# and to coerce `country` into something numeric and sensible
df$intl = 0
mask = which(df$country != 'United Kingdom')
df$intl[mask] = 1
df$country = NULL
```

\begin{center}\textbf{Figure 1.1 - Data preparation}\end{center}

\hrulefill

```{r}
# scale and print summary
df = scale(df)
row.names(df) = NULL
summary(df)
```

\begin{center}\textbf{Figure 1.2 - Summary statistics}\end{center}

\hrulefill

```{r}
# K-means clustering, k=2
set.seed(7)
km.2 = kmeans(df, 2, nstart=50)

cat('K-means clustering with', length(km.2$size), 'clusters of size', km.2$size[1], ',', km.2$size[2])

cat('\nIn percentage terms, the clusters make up ', round(km.2$size[1]/nrow(df), 3), round(km.2$size[2]/nrow(df), 3), 'of the data, respectively')

# cluster means
cat('\nCluster means:\n')
round(km.2$centers, 4) 
# within cluster sum of squares - variance in data set explained by clustering
# k-means minimizes within group dispersion and maximizes between group dispersion
cat('\nBetween ss / Total ss:\n')
round(km.2$betweenss/km.2$totss, 4)
```

\begin{center}\textbf{Figure 2.1 - K-means, k=2}\end{center}

\hrulefill

```{r}
# K-means clustering, k=3
km.3 = kmeans(df, 3, nstart=50)

cat('K-means clustering with', length(km.3$size), 'clusters of size', 
    km.3$size[1], ',', km.3$size[2], ',', km.3$size[3])

cat('\nIn percentage terms, the clusters make up ', 
    round(km.3$size[1]/nrow(df), 3), round(km.3$size[2]/nrow(df), 3), 
    round(km.3$size[3]/nrow(df), 3),
    'of the data, respectively')

# cluster means
cat('\nCluster means:\n')
round(km.3$centers, 4) 

# within cluster sum of squares - variance in data set explained by clustering
cat('\nBetween ss / Total ss:\n')
round(km.3$betweenss/km.3$totss, 4)
```

\begin{center}\textbf{Figure 2.2 - K-means, k=3}\end{center}

\hrulefill

```{r}
# K-means clustering, k=4
km.4 = kmeans(df, 4, nstart=50)

cat('K-means clustering with', length(km.4$size), 'clusters of size', 
    km.4$size[1], ',', km.4$size[2], ',', km.4$size[3], ',', km.4$size[4])

cat('\nIn percentage terms, the clusters make up ', 
    round(km.4$size[1]/nrow(df), 3), round(km.4$size[2]/nrow(df), 3), 
    round(km.4$size[3]/nrow(df), 3), round(km.4$size[4]/nrow(df), 3),
    'of the data, respectively')

# cluster means
cat('\nCluster means:\n')
round(km.4$centers, 4) 

# within cluster sum of squares - variance in data set explained by clustering
cat('\nBetween ss / Total ss:\n')
round(km.4$betweenss/km.4$totss, 4)
```

\begin{center}\textbf{Figure 2.3 - K-means, k=4}\end{center}

\hrulefill

```{r}
# K-means clustering, k=5
km.5 = kmeans(df, 5, nstart=50)

cat('K-means clustering with', length(km.5$size), 'clusters of size', 
    km.5$size[1], ',', km.5$size[2], ',', km.5$size[3], ',', km.5$size[4], ',', km.5$size[5])

cat('\nIn percentage terms, the clusters make up ', 
    round(km.5$size[1]/nrow(df), 3), round(km.5$size[2]/nrow(df), 3), 
    round(km.5$size[3]/nrow(df), 3), round(km.5$size[4]/nrow(df), 3),
    round(km.5$size[5]/nrow(df), 3),
    'of the data, respectively')

# cluster means
cat('\nCluster means:\n')
round(km.5$centers, 4) 

# within cluster sum of squares - variance in data set explained by clustering
cat('\nBetween ss / Total ss:\n')
round(km.5$betweenss/km.5$totss, 4)
```

\begin{center}\textbf{Figure 2.4 - K-means, k=5 (final)}\end{center}

\hrulefill


```{r}
# hierarchical clustering
df.dist = dist(df)

hcc = hclust(df.dist)
hca = hclust(df.dist, method='average')
hcs = hclust(df.dist, method='single')
hcce = hclust(df.dist, method='centroid')

plot(hcc, main="Complete Linkage", xlab="", ylab="", sub="")
```

\begin{center}\textbf{Figure 3.1 - Dendrogram, complete linkage}\end{center}

\hrulefill

```{r}
hcc.clusters.10 = cutree(hcc, 10)
table(hcc.clusters.10)

hcc.clusters.9 = cutree(hcc, 9)
table(hcc.clusters.9)

hcc.clusters.8 = cutree(hcc, 8)
table(hcc.clusters.8)

hcc.clusters.7 = cutree(hcc, 7)
table(hcc.clusters.7)

hcc.clusters.6 = cutree(hcc, 6)
table(hcc.clusters.6)

hcc.clusters.5 = cutree(hcc, 5)
table(hcc.clusters.5)

hcc.clusters.4 = cutree(hcc, 4)
table(hcc.clusters.4)

hcc.clusters.3 = cutree(hcc, 3)
table(hcc.clusters.3)
```

\begin{center}\textbf{Figure 3.2 - Complete linkage, k=3 to 10}\end{center}

\hrulefill

```{r}
plot(hca, main="Average Linkage", xlab="", ylab="", sub="")
```

\begin{center}\textbf{Figure 3.3 - Dendrogram, average linkage}\end{center}

\hrulefill

```{r}
hca.clusters.10 = cutree(hca, 10)
table(hca.clusters.10)

hca.clusters.9 = cutree(hca, 9)
table(hca.clusters.9)

hca.clusters.8 = cutree(hca, 8)
table(hca.clusters.8)

hca.clusters.7 = cutree(hca, 7)
table(hca.clusters.7)

hca.clusters.6 = cutree(hca, 6)
table(hca.clusters.6)

hca.clusters.5 = cutree(hca, 5)
table(hca.clusters.5)

hca.clusters.4 = cutree(hca, 4)
table(hca.clusters.4)

hca.clusters.3 = cutree(hca, 3)
table(hca.clusters.3)
```

\begin{center}\textbf{Figure 3.4 - Average linkage, k=3 to 10}\end{center}

\hrulefill

```{r}
plot(hcs, main="Single Linkage", xlab="", ylab="", sub="")
```

\begin{center}\textbf{Figure 3.5 - Dendrogram, single linkage}\end{center}

\hrulefill

```{r}
hcs.clusters.10 = cutree(hcs, 10)
table(hcs.clusters.10)

hcs.clusters.9 = cutree(hcs, 9)
table(hcs.clusters.9)

hcs.clusters.8 = cutree(hcs, 8)
table(hcs.clusters.8)

hcs.clusters.7 = cutree(hcs, 7)
table(hcs.clusters.7)

hcs.clusters.6 = cutree(hcs, 6)
table(hcs.clusters.6)

hcs.clusters.5 = cutree(hcs, 5)
table(hcs.clusters.5)

hcs.clusters.4 = cutree(hcs, 4)
table(hcs.clusters.4)

hcs.clusters.3 = cutree(hcs, 3)
table(hcs.clusters.3)
```

\begin{center}\textbf{Figure 3.6 - Single linkage, k=3 to 10}\end{center}

\hrulefill

```{r}
plot(hcce, main="Centroid Linkage", xlab="", ylab="", sub="")
```

\begin{center}\textbf{Figure 3.7 - Dendrogram, centroid linkage}\end{center}

\hrulefill


```{r}
hcce.clusters.10 = cutree(hcce, 10)
table(hcce.clusters.10)

hcce.clusters.9 = cutree(hcce, 9)
table(hcce.clusters.9)

hcce.clusters.8 = cutree(hcce, 8)
table(hcce.clusters.8)

hcce.clusters.7 = cutree(hcce, 7)
table(hcce.clusters.7)

hcce.clusters.6 = cutree(hcce, 6)
table(hcce.clusters.6)

hcce.clusters.5 = cutree(hcce, 5)
table(hcce.clusters.5)

hcce.clusters.4 = cutree(hcce, 4)
table(hcce.clusters.4)

hcce.clusters.3 = cutree(hcce, 3)
table(hcce.clusters.3)
```

\begin{center}\textbf{Figure 3.8 - Centroid linkage, k=3 to 10}\end{center}

\hrulefill

```{r}
# example of using hclust to visualize k=5 clusters for `hcc`
plot(hcc, labels=F, main="Complete Linkage", xlab="", ylab="", sub="")
rect.hclust(hcc, k=5)
```

\begin{center}\textbf{Figure 3.9 - Complete linkage, k=5 visualized}\end{center}

\hrulefill

# 4. Conclusions

In this assignment I used k-means and hierarchical clustering to try to examine how customers of this business could be grouped together. I found that k-means with k=5 resulted in better separation than any hierarchical method and tried to make sense of the five distinct clusters, pointing out that two of them made up a huge amount of the business' patronage and should be focused on moving forward. Of course, whether or not one algorithm is more appropriate than another can largely depend on the data, but in this case, k-means resulted in more varied, interesting, and informative clusters than hierarchical clustering, which tended to return one large cluster and many tiny clusters, even with *k* as high as 10.

# 5. Appendix

```{r ref.label=knitr::all_labels(), echo=T, eval=F}
```