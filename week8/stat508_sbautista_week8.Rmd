---
title: "STAT 508 Data Analysis Assignment 8"
author: "Sebastian Bautista"
output:
  pdf_document: default
---

```{r setup, include=FALSE, cache=T}
knitr::opts_chunk$set(comment='>')

library(readxl)
eretail = read_excel("Online Retail.xlsx")
dim(eretail)
names(eretail)

eretail = eretail[eretail$Country != "Unspecified",] # remove 'unspecified' country
eretail = eretail[eretail$Quantity > 0,]             # remove returns/cancellations

IDtab = table(eretail$Country, eretail$CustomerID)   # crosstab country by customer ID
IDtab = apply(IDtab >0, 2, sum)                      # is any customer ID duplicated across countries?
duplicateIDs = names(IDtab[IDtab > 1])               # duplicate IDs to clean up
eretail = eretail[!is.element(eretail$CustomerID, duplicateIDs),]
rm(IDtab)

eretail$InvoiceMth = substr(eretail$InvoiceDate, 1, 7)         # extract month of invoice
eretail = eretail[as.vector(eretail$InvoiceMth) != "2011-12",] # remove December 2011 as it only covers first week

eretail$Amount = eretail$Quantity * eretail$UnitPrice           # compute amount per invoice item

eaggr = aggregate(Amount~Country+CustomerID, data=eretail, sum) # compute aggregate amount spent per customer
row.names(eaggr) = eaggr$CustomerID
eaggr = eaggr[,-2]
eaggr = cbind(eaggr, aggregate(InvoiceMth~CustomerID, data=eretail, min)[,-1]) # 1st month of customer interaction
names(eaggr)[3] = "FirstMth"
eaggr = cbind(eaggr, aggregate(InvoiceMth~CustomerID, data=eretail, max)[,-1]) # last month of cust. interaction
names(eaggr)[4] = "LastMth"

# relabel months and compute duration of customer interaction
levels(eaggr$FirstMth) = 1:12
levels(eaggr$LastMth) = 1:12
eaggr$FirstMth = as.numeric(as.vector(eaggr$FirstMth))
eaggr$LastMth = as.numeric(as.vector(eaggr$LastMth))
eaggr$Months = eaggr$LastMth - eaggr$FirstMth + 1

eaggr = cbind(eaggr, apply( table(eretail$CustomerID, eretail$InvoiceMth) , 1, sum ) )
names(eaggr)[6] = "Purchases"

# Some useful statistics (which you may or may not decide to use)
eaggr$Amount.per.Purchase = eaggr$Amount / eaggr$Purchases
eaggr$Purchases.per.Month = eaggr$Purchases / eaggr$Months
eaggr$Amount.per.Month = eaggr$Amount / eaggr$Months

eaggr[1:30,]

df = eaggr
colnames(df) = tolower(colnames(df))
```

# 0. Introduction

This week, we're using clustering techniques on sales data from a UK-based company. The original data is at the transaction level, but we aggregate to the customer level and create a few more variables in order to do our analysis. I begin by changing the data into a form that clustering algorithms can work with. Then, I try out k-means clustering and hierarchical clustering, varying values of *k* for both algorithms and methods of calculating distance for `hclust`. I find that k-means results in much more interesting and informative clusters compared to hierarchical clustering, regardless of whether complete, average, single, or centroid linkage is used for `hclust`. 

# 1. Data

The data set created after aggregation, `eaggr`, contains 4286 observations and 9 variables. Each observation represents a customer and each row is indexed by customer ID. Variables include the customer's country, the total amount of pounds spent (quantity times price), the first and last months of customer interaction, and the number of purchases by a customer, as well as the amount of pounds spent per purchase, the number of purchases per month, and the amount of pounds spent per month. All but `country` are numeric; in **figure 1.1** I use this character variable to create a dummy variable called `intl` which is equal to 1 if the customer is outside of the UK and 0 otherwise. 

# 2. Analyses

*Using cluster analysis, your aim is to identify whether this company's consumers can be segmented meaningfully by the recency and duration of their interaction with the company, the frequency of their purchases/orders, and the amount they spent (amounts are in Sterling pounds). Further segmentation by country would also be very helpful for marketing purposes.*

Recency - `lastmth`
Duration - `months`
Frequency - `purchases`, `purchases.per.month`
Amount - `amount`, `amount.per.purchase`, `amount.per.month`

*For the data source, documentation, and an interesting study using a variant of this data, please see https://archive.ics.uci.edu/ml/datasets/Online%20Retail*

# 3. Plots and Tables

```{r}
cat('The data contains', length(unique(df$country)), 'unique countries\n')
table(df$country)

# create `intl` dummy variable because of predominance of UK customers
# and to coerce `country` into something numeric and sensible
df$intl = 0
mask = which(df$country != 'United Kingdom')
df$intl[mask] = 1
df$country = NULL
```

\begin{center}\textbf{Figure 1.1 - Data preparation}\end{center}

\hrulefill

```{r}
# scale and print summary
df = scale(df)
row.names(df) = NULL
summary(df)
```

\begin{center}\textbf{Figure 1.2 - Summary statistics}\end{center}

\hrulefill

```{r}
# K-means clustering, k=2
set.seed(7)
km.2 = kmeans(df, 2, nstart=50)

cat('K-means clustering with', length(km.2$size), 'clusters of size', km.2$size[1], ',', km.2$size[2])

cat('\nIn percentage terms, the clusters make up ', round(km.2$size[1]/nrow(df), 3), round(km.2$size[2]/nrow(df), 3), 'of the data, respectively')

# cluster means
cat('\n\nCluster means:\n')
round(km.2$centers, 4) 
# within cluster sum of squares - variance in data set explained by clustering
# k-means minimizes within group dispersion and maximizes between group dispersion
# by assigning the obs to these clusters rather than n clusters,
# we achieve a reduction in sum of squares of 17.3%
cat('\nBetween ss / Total ss:\n')
round(km.2$betweenss/km.2$totss, 4)
```

\begin{center}\textbf{Figure 2.1 - K-means, k=2}\end{center}

\hrulefill

```{r}
# K-means clustering, k=3
km.3 = kmeans(df, 3, nstart=50)

cat('K-means clustering with', length(km.3$size), 'clusters of size', 
    km.3$size[1], ',', km.3$size[2], ',', km.3$size[3])

cat('\nIn percentage terms, the clusters make up ', 
    round(km.3$size[1]/nrow(df), 3), round(km.3$size[2]/nrow(df), 3), 
    round(km.3$size[3]/nrow(df), 3),
    'of the data, respectively')

# cluster means
cat('\n\nCluster means:\n')
round(km.3$centers, 4) 

# within cluster sum of squares - variance in data set explained by clustering
cat('\nBetween ss / Total ss:\n')
round(km.3$betweenss/km.3$totss, 4)
```

\begin{center}\textbf{Figure 2.2 - K-means, k=3}\end{center}

\hrulefill

```{r}
# K-means clustering, k=4
km.4 = kmeans(df, 4, nstart=50)

cat('K-means clustering with', length(km.4$size), 'clusters of size', 
    km.4$size[1], ',', km.4$size[2], ',', km.4$size[3], ',', km.4$size[4])

cat('\nIn percentage terms, the clusters make up ', 
    round(km.4$size[1]/nrow(df), 3), round(km.4$size[2]/nrow(df), 3), 
    round(km.4$size[3]/nrow(df), 3), round(km.4$size[4]/nrow(df), 3),
    'of the data, respectively')

# cluster means
cat('\n\nCluster means:\n')
round(km.4$centers, 4) 

# within cluster sum of squares - variance in data set explained by clustering
cat('\nBetween ss / Total ss:\n')
round(km.4$betweenss/km.4$totss, 4)
```

\begin{center}\textbf{Figure 2.3 - K-means, k=4}\end{center}

\hrulefill

```{r}
# K-means clustering, k=5
# might stop here because we get a singleton observation
km.5 = kmeans(df, 5, nstart=50)

cat('K-means clustering with', length(km.5$size), 'clusters of size', 
    km.5$size[1], ',', km.5$size[2], ',', km.5$size[3], ',', km.5$size[4], ',', km.5$size[5])

cat('\nIn percentage terms, the clusters make up ', 
    round(km.5$size[1]/nrow(df), 3), round(km.5$size[2]/nrow(df), 3), 
    round(km.5$size[3]/nrow(df), 3), round(km.5$size[4]/nrow(df), 3),
    round(km.5$size[5]/nrow(df), 3),
    'of the data, respectively')

# cluster means
cat('\n\nCluster means:\n')
round(km.5$centers, 4) 

# within cluster sum of squares - variance in data set explained by clustering
cat('\nBetween ss / Total ss:\n')
round(km.5$betweenss/km.5$totss, 4)
```

\begin{center}\textbf{Figure 2.4 - K-means, k=5 (final)}\end{center}

\hrulefill


```{r}
# hierarchical clustering
# complete, average, and centroid look ok but single looks terrible
# will show it here, but compare k-means to complete and average later
df.dist = dist(df)

hcc = hclust(df.dist)
hca = hclust(df.dist, method='average')
hcs = hclust(df.dist, method='single')
hcce = hclust(df.dist, method='centroid')

plot(hcc, main="Complete Linkage", xlab="", ylab="", sub="")
plot(hca, main="Average Linkage", xlab="", ylab="", sub="")
plot(hcs, main="Single Linkage", xlab="", ylab="", sub="")
plot(hcce, main="Centroid Linkage", xlab="", ylab="", sub="")
```

\begin{center}\textbf{Figure 3.1 - Hierarchical clustering and dendograms}\end{center}

\hrulefill

```{r}
hcc.clusters.10 = cutree(hcc, 10)
table(hcc.clusters.10)

hcc.clusters.9 = cutree(hcc, 9)
table(hcc.clusters.9)

hcc.clusters.8 = cutree(hcc, 8)
table(hcc.clusters.8)

hcc.clusters.7 = cutree(hcc, 7)
table(hcc.clusters.7)

hcc.clusters.6 = cutree(hcc, 6)
table(hcc.clusters.6)

hcc.clusters.5 = cutree(hcc, 5)
table(hcc.clusters.5)

hcc.clusters.4 = cutree(hcc, 4)
table(hcc.clusters.4)

hcc.clusters.3 = cutree(hcc, 3)
table(hcc.clusters.3)

# looks pretty weird. probably not the way to go
# anything between k=3 and k=10 leads to one huge cluster and lots of small ones
```

\begin{center}\textbf{Figure 3.2 - Complete linkage, k=3 to 10}\end{center}

\hrulefill

```{r}
hca.clusters.10 = cutree(hca, 10)
table(hca.clusters.10)

hca.clusters.9 = cutree(hca, 9)
table(hca.clusters.9)

hca.clusters.8 = cutree(hca, 8)
table(hca.clusters.8)

hca.clusters.7 = cutree(hca, 7)
table(hca.clusters.7)

hca.clusters.6 = cutree(hca, 6)
table(hca.clusters.6)

hca.clusters.5 = cutree(hca, 5)
table(hca.clusters.5)

hca.clusters.4 = cutree(hca, 4)
table(hca.clusters.4)

hca.clusters.3 = cutree(hca, 3)
table(hca.clusters.3)
# same as above
```

\begin{center}\textbf{Figure 3.3 - Average linkage, k=3 to 10}\end{center}

\hrulefill

```{r}
hcs.clusters.10 = cutree(hcs, 10)
table(hcs.clusters.10)

hcs.clusters.9 = cutree(hcs, 9)
table(hcs.clusters.9)

hcs.clusters.8 = cutree(hcs, 8)
table(hcs.clusters.8)

hcs.clusters.7 = cutree(hcs, 7)
table(hcs.clusters.7)

hcs.clusters.6 = cutree(hcs, 6)
table(hcs.clusters.6)

hcs.clusters.5 = cutree(hcs, 5)
table(hcs.clusters.5)

hcs.clusters.4 = cutree(hcs, 4)
table(hcs.clusters.4)

hcs.clusters.3 = cutree(hcs, 3)
table(hcs.clusters.3)
# again same as above and even worse, with tons of singletons
```

\begin{center}\textbf{Figure 3.4 - Single linkage, k=3 to 10}\end{center}

\hrulefill

```{r}
hcce.clusters.10 = cutree(hcce, 10)
table(hcce.clusters.10)

hcce.clusters.9 = cutree(hcce, 9)
table(hcce.clusters.9)

hcce.clusters.8 = cutree(hcce, 8)
table(hcce.clusters.8)

hcce.clusters.7 = cutree(hcce, 7)
table(hcce.clusters.7)

hcce.clusters.6 = cutree(hcce, 6)
table(hcce.clusters.6)

hcce.clusters.5 = cutree(hcce, 5)
table(hcce.clusters.5)

hcce.clusters.4 = cutree(hcce, 4)
table(hcce.clusters.4)

hcce.clusters.3 = cutree(hcce, 3)
table(hcce.clusters.3)
# no improvement. go with kmeans
```

\begin{center}\textbf{Figure 3.5 - Centroid linkage, k=3 to 10}\end{center}

\hrulefill

```{r}
# example of using hclust to visualize k=5 clusters for `hcc`
plot(hcc, labels=F, main="Complete Linkage", xlab="", ylab="", sub="")
rect.hclust(hcc, k=5)
```

\begin{center}\textbf{Figure 3.6 - Complete linkage, k=5 visualized}\end{center}

\hrulefill

# 4. Conclusions

# 5. Appendix

```{r ref.label=knitr::all_labels(), echo=T, eval=F}
```