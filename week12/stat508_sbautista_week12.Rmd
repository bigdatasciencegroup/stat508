---
title: "STAT 508 Data Analysis Assignment 12"
author: "Sebastian Bautista"
output:
  pdf_document: default
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(comment='>', echo=F)
options(digits=2)

library(ISLR)
library(e1071)
library(ROCR)
df = OJ
```

# 0. Introduction

In this assignment, I fit SVMs with linear, radial, and polynomial kernels, calculate error rates, tune them using 10-fold cross-validation, then compare their overall performance. 

# 1. Data

The data contains 1070 observations and 18 variables, where each observation is a purchase and each variable is a characteristic of the customer or product. Variables include things like price charged for each of the two brands of orange juice, discounts, which store the sale occurred at, and customer brand loyalty. 

# 2. Analyses

*1. Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.*

I split the data set into training and test sets using the `sample()` function in **figure 1.1**. 

*2. Fit a support vector classifier to the training data using cost=0.01, with Purchase as the response and the other variables as predictors. Use the summary() function to produce summary statistics, and describe the results obtained.*

**Figure 1.2** contains the training set results from fitting a support vector classifier. There are 439 support vectors out of 800 data points, so more than half of the data points are used to support the hyperplanes. This may indicate high variance or overfitting, as the hyperplane is supported by relatively many data points, and will therefore be more sensitive to changes in the data compared to a fit with fewer support vectors.

*3. What are the training and test error rates?*

Confusion matrices and calculated rates resulting from predicting on the training and test data are in **figure 1.3**. Using the training data, 138/800 or 17.25% of the observations are misclassified. Predicting on the test data, 49/270 or 18.15% of the observations are misclassified.

*4. Use the tune() function to select an optimal cost. Consider values in the range 0.01 to 10.*

Using 10-fold cross-validation in **figure 1.4**, I find that the optimal cost for SVC is 5. However, the error is very close for each of the models fitted, at around 0.17. 

*5. Compute the training and test error rates using this new value for cost.*

The results from using the cross-validated model are in **figure 1.5**. With 123/800 of the observations in the training set misclassified, the training error rate for this model is 15.38%. 44/270 of the test observations are misclassified, or 16.29%. These both show an improvement over the untuned model with cost=0.01. The higher value for cost here represents a larger budget for accepting misclassifications in favor of a larger margin, which typically leads to lower variance and higher bias and better out-of-sample performance. This can be seen in another way when looking at the summary of the cross-validated model; there are 350 support vectors here, down from 439, implying that this model will be less sensitive to changes in the data compared to the untuned model.

*6. Repeat parts 2 through 5 using a support vector machine with a radial kernel. Use the default value for gamma.*

**Figure 2.1** shows that the untuned radial SVM results in 629 support vectors, signalling even more sensitivity to changes in the data than the untuned SVC. Calculating confusion matrices for the training and test data in **figure 2.2** reveals that this SVM predicts that every orange juice purchase is Citrus Hill brand and none are Minute Maid. This results in low accuracy, recall of 0 (where MM is the 'positive class'), specificity of 100% (where CH is the 'negative class'), and undefined precision. 

I tune the radial SVM in **figure 2.3**, settling on cost=5 and and error rate of 0.17, which is the same as the tuned SVC. Using the best model output by `tune()`, in **figure 2.4** I show that the training error rate is now 13%, the test error rate is 18.14%, and the number of support vectors is 333. 

*7. Repeat parts 2 through 5 using a support vector machine with a polynomial kernel. Set degree=2.*

Results from the untuned polynomial SVM are in **figure 3.1**. 

*8. Overall, which approach seems to give the best results on this data?*

# 3. Plots and Tables

```{r, echo=T}
set.seed(202)
train = sample(1:nrow(df), 800)
df.train = df[train,]
df.test = df[-train,]
```
\begin{center}\textbf{Figure 1.1 - Train-test split}\end{center}
\hrulefill

```{r}
svm.l = svm(Purchase ~ ., data=df.train, kernel='linear', cost=0.01, scale=T)
summary(svm.l)
```
\begin{center}\textbf{Figure 1.2 - Untuned SVC}\end{center}
\hrulefill

```{r}
calc.rates <- function(yhat, ytest){
  # calculates rates from confusion matrix
  # from predicted and true labels
  cm = table(yhat, ytest)
  
  accuracy = (cm[2,2]+cm[1,1])/sum(cm)
  error.rate = 1 - accuracy
  recall = cm[2,2]/sum(cm[,2])
  specificity = cm[1,1]/sum(cm[,1])
  precision = cm[2,2]/sum(cm[2,])
  
  cat('\n')
  print(cm)
  
  cat('\nAccuracy: ', accuracy)
  cat('\nError rate: ', error.rate)
  cat('\nRecall: ', recall)
  cat('\nSpecificity: ', specificity)
  cat('\nPrecision: ', precision)
  cat('\n')
}

# training error
yhat.train = predict(svm.l, df.train)
ytrain = df.train$Purchase
calc.rates(yhat.train, ytrain)

# test error
yhat = predict(svm.l, df.test)
ytest = df.test$Purchase
calc.rates(yhat, ytest)
```
\begin{center}\textbf{Figure 1.3 - Results from untuned SVC}\end{center}
\hrulefill

```{r}
tune.out = tune(svm, Purchase ~ ., data=df.train, kernel='linear',
                ranges=list(cost=c(0.01, 0.1, 0.25, 0.5, 0.75, 1, 3, 5, 7, 10)))

# check out CV error for each model
summary(tune.out)
```
\begin{center}\textbf{Figure 1.4 - Cross-validation for SVC}\end{center}
\hrulefill

```{r}
bestsvm.l = tune.out$best.model

summary(bestsvm.l)

# training error
yhat.train = predict(bestsvm.l, df.train)
calc.rates(yhat.train, ytrain)

# test error
yhat = predict(bestsvm.l, df.test)
calc.rates(yhat, ytest)
```
\begin{center}\textbf{Figure 1.5 - Results from tuned SVC}\end{center}
\hrulefill

```{r}
# 6. Repeat parts 2 through 5 using a support vector machine with a radial kernel. Use the default value for gamma.
# 2. Fit a support vector classifier to the training data using cost=0.01, with Purchase as the response and the other variables as predictors. Use the summary() function to produce summary statistics, and describe the results obtained.

svm.r = svm(Purchase ~ ., data=df.train, kernel='radial', cost=0.01, scale=T)
summary(svm.r)
```
\begin{center}\textbf{Figure 2.1 - Untuned radial SVM}\end{center}
\hrulefill

```{r}
# 3. What are the training and test error rates?

# training error
yhat.train = predict(svm.r, df.train)
ytrain = df.train$Purchase
calc.rates(yhat.train, ytrain)

# test error
yhat = predict(svm.r, df.test)
ytest = df.test$Purchase
calc.rates(yhat, ytest)
```
\begin{center}\textbf{Figure 2.2 - Results from untuned radial SVM}\end{center}
\hrulefill

```{r}
# 4. Use the tune() function to select an optimal cost. Consider values in the range 0.01 to 10.
tune.out = tune(svm, Purchase ~ ., data=df.train, kernel='radial',
                ranges=list(cost=c(0.01, 0.1, 0.25, 0.5, 0.75, 1, 3, 5, 7, 10)))

# check out CV error for each model
summary(tune.out)
```
\begin{center}\textbf{Figure 2.3 - Cross-validation for radial SVM}\end{center}
\hrulefill

```{r}
# 5. Compute the training and test error rates using this new value for cost.

bestsvm.r = tune.out$best.model

summary(bestsvm.r)

# training error
yhat.train = predict(bestsvm.r, df.train)
calc.rates(yhat.train, ytrain)

# test error
yhat = predict(bestsvm.r, df.test)
calc.rates(yhat, ytest)
```
\begin{center}\textbf{Figure 2.4 - Results from tuned radial SVM}\end{center}
\hrulefill

```{r}
# 7. Repeat parts 2 through 5 using a support vector machine with a polynomial kernel. Set degree=2.
# 2. Fit a support vector classifier to the training data using cost=0.01, with Purchase as the response and the other variables as predictors. Use the summary() function to produce summary statistics, and describe the results obtained.

svm.p = svm(Purchase ~ ., data=df.train, kernel='polynomial', cost=0.01, degree=2, scale=T)
summary(svm.p)
```
\begin{center}\textbf{Figure 3.1 - Untuned polynomial SVM}\end{center}
\hrulefill

```{r}
# 3. What are the training and test error rates?

# training error
yhat.train = predict(svm.p, df.train)
ytrain = df.train$Purchase
calc.rates(yhat.train, ytrain)

# test error
yhat = predict(svm.p, df.test)
ytest = df.test$Purchase
calc.rates(yhat, ytest)
```
\begin{center}\textbf{Figure 3.2 - Results from untuned polynomial SVM}\end{center}
\hrulefill

```{r}
# 4. Use the tune() function to select an optimal cost. Consider values in the range 0.01 to 10.
tune.out = tune(svm, Purchase ~ ., data=df.train, kernel='polynomial', degree=2,
                ranges=list(cost=c(0.01, 0.1, 0.25, 0.5, 0.75, 1, 3, 5, 7, 10)))

# check out CV error for each model
summary(tune.out)
```
\begin{center}\textbf{Figure 3.3 - Cross-validation for polynomial SVM}\end{center}
\hrulefill

```{r}
# 5. Compute the training and test error rates using this new value for cost.

bestsvm.p = tune.out$best.model

# training error
yhat.train = predict(bestsvm.p, df.train)
calc.rates(yhat.train, ytrain)

# test error
yhat = predict(bestsvm.p, df.test)
calc.rates(yhat, ytest)
```
\begin{center}\textbf{Figure 3.4 - Results from tuned polynomial SVM}\end{center}
\hrulefill

```{r}
# 8. Overall, which approach seems to give the best results on this data?

rocplot = function(yhat, ytest, ...){
  predob = prediction(yhat, ytest)
  perf = performance(predob, "tpr", "fpr")
  plot(perf, ...)
  auc = performance(predob, measure='auc')
  cat('AUC: ', auc@y.values[[1]])
  cat('\n')
}

fitted.l = attributes(predict(bestsvm.l, df[-train,], decision.values=T))$decision.values
rocplot(fitted.l, df[-train,'Purchase'], main="SVM, test data")

fitted.r = attributes(predict(bestsvm.r, df[-train,], decision.values=T))$decision.values
rocplot(fitted.r, df[-train,'Purchase'], add=T, col='blue')

fitted.p = attributes(predict(bestsvm.p, df[-train,], decision.values=T))$decision.values
rocplot(fitted.p, df[-train,'Purchase'], add=T, col='darkorange')
```
\begin{center}\textbf{Figure 4 - ROC plots and AUROC for all SVMs}\end{center}
\hrulefill

# 4. Conclusions

# 5. Appendix

```{r ref.label=knitr::all_labels(), echo=T, eval=F}
```