---
title: "STAT 508 Data Analysis Assignment 12"
author: "Sebastian Bautista"
output:
  pdf_document: default
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(comment='>', echo=F)
options(digits=2)

library(ISLR)
library(e1071)
library(ROCR)
df = OJ
```

# 0. Introduction

In this assignment, I fit SVMs with linear, radial, and polynomial kernels, calculate error rates, tune them using 10-fold cross-validation, then compare their overall performance. The three models perform very differently when untuned, but when tuned, they come up with fairly similar predictions. I end up calculating ROC curves and the area under the curves, finding that radial SVM is the best for classifying orange juice purchases, though all three have similar performance on the test set. 

# 1. Data

The data contains 1070 observations and 18 variables, where each observation is an orange juice purchase and each variable is a characteristic of the customer or product. Variables include things like price charged for each of the two brands of orange juice, discounts, which store the sale occurred at, and customer brand loyalty. 

# 2. Analyses

*1. Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.*

I split the data set into training and test sets using the `sample()` function in **figure 1.1**. 

*2. Fit a support vector classifier to the training data using cost=0.01, with Purchase as the response and the other variables as predictors. Use the summary() function to produce summary statistics, and describe the results obtained.*

**Figure 1.2** contains the training set results from fitting a support vector classifier. There are 439 support vectors out of 800 data points, so more than half of the data points are used to support the hyperplanes. This may indicate high variance or overfitting, as the hyperplane is supported by relatively many data points, and will therefore be more sensitive to changes in the data compared to a fit with fewer support vectors.

*3. What are the training and test error rates?*

Confusion matrices and calculated rates resulting from predicting on the training and test data are in **figure 1.3**. Using the training data, 138/800 or 17.25% of the observations are misclassified. Predicting on the test data, 49/270 or 18.15% of the observations are misclassified.

*4. Use the tune() function to select an optimal cost. Consider values in the range 0.01 to 10.*

Using 10-fold cross-validation in **figure 1.4**, I find that the optimal cost for SVC is 5. However, the error is very close for each of the models fitted, at around 0.17. 

*5. Compute the training and test error rates using this new value for cost.*

The results from using the cross-validated model are in **figure 1.5**. With 123/800 of the observations in the training set misclassified, the training error rate for this model is 15.38%. 44/270 of the test observations are misclassified, or 16.29%. These both show an improvement over the untuned model with cost=0.01. The higher value for cost here represents a larger budget for accepting misclassifications in favor of a larger margin, which typically leads to lower variance and higher bias and better out-of-sample performance. This can be seen in another way when looking at the summary of the cross-validated model; there are 350 support vectors here, down from 439, implying that this model will be less sensitive to changes in the data compared to the untuned model.

*6. Repeat parts 2 through 5 using a support vector machine with a radial kernel. Use the default value for gamma.*

**Figure 2.1** shows that the untuned radial SVM results in 629 support vectors, signalling even more sensitivity to changes in the data than the untuned SVC. Calculating confusion matrices for the training and test data in **figure 2.2** reveals that this SVM predicts that every orange juice purchase is Citrus Hill brand and none are Minute Maid. This results in low accuracy, recall of 0 (where MM is the 'positive class'), specificity of 100% (where CH is the 'negative class'), and undefined precision. 

I tune the radial SVM in **figure 2.3**, settling on cost=5 and and error rate of 0.17, which is the same as the tuned SVC. Using the best model output by `tune()`, in **figure 2.4** I show that the training error rate is now 13%, the test error rate is 18.14%, and the number of support vectors is 333. 

*7. Repeat parts 2 through 5 using a support vector machine with a polynomial kernel. Set degree=2.*

Results from the untuned polynomial SVM are in **figure 3.1**. For this fit, we have even more support vectors than for the radial SVM, at 633. **Figure 3.2** shows the training and test results from the untuned SVM; similarly to the previous run, it overwhelmingly predicts Citrus Hill for both data sets, which results in a high error rate and poor recall. 

Tuning this SVM in **figure 3.3** allows us to settle on cost=7 with an error rate of 0.17, the same error rate we saw in the other runs. This tells us that these three different SVMs, when tuned, have similar performance in cross-validation and may also have similar performance on the test set. **Figure 3.4** contains the training and test set results from the tuned polynomial SVM. The training error rate is 14.75%, the test error rate is 16.67%, and the number of support vectors is 350.

*8. Overall, which approach seems to give the best results on this data?*

In order to have a more general view of how the different approaches perform, I plot ROC curves for all three fitted models using the test data in **figure 4**. SVC is in black, radial SVM is in blue, and polynomial SVM is in orange. As hinted by the identical cross-validation error, the three models perform similarly, which leads to slight differences in the ROC curves. To finally decide on the winner, I calculate the area under the curve (AUROC), which is also reported in the figure. With an AUROC of 0.12, radial SVM just barely beats out polynomial and linear SVM.

# 3. Plots and Tables

```{r, echo=T}
set.seed(202)
train = sample(1:nrow(df), 800)
df.train = df[train,]
df.test = df[-train,]
```
\begin{center}\textbf{Figure 1.1 - Train-test split}\end{center}
\hrulefill

```{r}
svm.l = svm(Purchase ~ ., data=df.train, kernel='linear', cost=0.01, scale=T)
summary(svm.l)
```
\begin{center}\textbf{Figure 1.2 - Untuned SVC}\end{center}
\hrulefill

```{r}
calc.rates <- function(yhat, ytest){
  # calculates rates from confusion matrix
  # from predicted and true labels
  cm = table(yhat, ytest)
  
  accuracy = (cm[2,2]+cm[1,1])/sum(cm)
  error.rate = 1 - accuracy
  recall = cm[2,2]/sum(cm[,2])
  specificity = cm[1,1]/sum(cm[,1])
  precision = cm[2,2]/sum(cm[2,])
  
  cat('\n')
  print(cm)
  
  cat('\nAccuracy: ', accuracy)
  cat('\nError rate: ', error.rate)
  cat('\nRecall: ', recall)
  cat('\nSpecificity: ', specificity)
  cat('\nPrecision: ', precision)
  cat('\n')
}

# training error
yhat.train = predict(svm.l, df.train)
ytrain = df.train$Purchase
calc.rates(yhat.train, ytrain)

# test error
yhat = predict(svm.l, df.test)
ytest = df.test$Purchase
calc.rates(yhat, ytest)
```
\begin{center}\textbf{Figure 1.3 - Results from untuned SVC}\end{center}
\hrulefill

```{r}
tune.out = tune(svm, Purchase ~ ., data=df.train, kernel='linear',
                ranges=list(cost=c(0.01, 0.1, 0.25, 0.5, 0.75, 1, 3, 5, 7, 10)))

# check out CV error for each model
summary(tune.out)
```
\begin{center}\textbf{Figure 1.4 - Cross-validation for SVC}\end{center}
\hrulefill

```{r}
bestsvm.l = tune.out$best.model

summary(bestsvm.l)

# training error
yhat.train = predict(bestsvm.l, df.train)
calc.rates(yhat.train, ytrain)

# test error
yhat = predict(bestsvm.l, df.test)
calc.rates(yhat, ytest)
```
\begin{center}\textbf{Figure 1.5 - Results from tuned SVC}\end{center}
\hrulefill

```{r}
# 6. Repeat parts 2 through 5 using a support vector machine with a radial kernel. Use the default value for gamma.
# 2. Fit a support vector classifier to the training data using cost=0.01, with Purchase as the response and the other variables as predictors. Use the summary() function to produce summary statistics, and describe the results obtained.

svm.r = svm(Purchase ~ ., data=df.train, kernel='radial', cost=0.01, scale=T)
summary(svm.r)
```
\begin{center}\textbf{Figure 2.1 - Untuned radial SVM}\end{center}
\hrulefill

```{r}
# 3. What are the training and test error rates?

# training error
yhat.train = predict(svm.r, df.train)
ytrain = df.train$Purchase
calc.rates(yhat.train, ytrain)

# test error
yhat = predict(svm.r, df.test)
ytest = df.test$Purchase
calc.rates(yhat, ytest)
```
\begin{center}\textbf{Figure 2.2 - Results from untuned radial SVM}\end{center}
\hrulefill

```{r}
# 4. Use the tune() function to select an optimal cost. Consider values in the range 0.01 to 10.
tune.out = tune(svm, Purchase ~ ., data=df.train, kernel='radial',
                ranges=list(cost=c(0.01, 0.1, 0.25, 0.5, 0.75, 1, 3, 5, 7, 10)))

# check out CV error for each model
summary(tune.out)
```
\begin{center}\textbf{Figure 2.3 - Cross-validation for radial SVM}\end{center}
\hrulefill

```{r}
# 5. Compute the training and test error rates using this new value for cost.

bestsvm.r = tune.out$best.model

summary(bestsvm.r)

# training error
yhat.train = predict(bestsvm.r, df.train)
calc.rates(yhat.train, ytrain)

# test error
yhat = predict(bestsvm.r, df.test)
calc.rates(yhat, ytest)
```
\begin{center}\textbf{Figure 2.4 - Results from tuned radial SVM}\end{center}
\hrulefill

```{r}
# 7. Repeat parts 2 through 5 using a support vector machine with a polynomial kernel. Set degree=2.
# 2. Fit a support vector classifier to the training data using cost=0.01, with Purchase as the response and the other variables as predictors. Use the summary() function to produce summary statistics, and describe the results obtained.

svm.p = svm(Purchase ~ ., data=df.train, kernel='polynomial', cost=0.01, degree=2, scale=T)
summary(svm.p)
```
\begin{center}\textbf{Figure 3.1 - Untuned polynomial SVM}\end{center}
\hrulefill

```{r}
# 3. What are the training and test error rates?

# training error
yhat.train = predict(svm.p, df.train)
ytrain = df.train$Purchase
calc.rates(yhat.train, ytrain)

# test error
yhat = predict(svm.p, df.test)
ytest = df.test$Purchase
calc.rates(yhat, ytest)
```
\begin{center}\textbf{Figure 3.2 - Results from untuned polynomial SVM}\end{center}
\hrulefill

```{r}
# 4. Use the tune() function to select an optimal cost. Consider values in the range 0.01 to 10.
tune.out = tune(svm, Purchase ~ ., data=df.train, kernel='polynomial', degree=2,
                ranges=list(cost=c(0.01, 0.1, 0.25, 0.5, 0.75, 1, 3, 5, 7, 10)))

# check out CV error for each model
summary(tune.out)
```
\begin{center}\textbf{Figure 3.3 - Cross-validation for polynomial SVM}\end{center}
\hrulefill

```{r}
# 5. Compute the training and test error rates using this new value for cost.

bestsvm.p = tune.out$best.model

summary(bestsvm.p)

# training error
yhat.train = predict(bestsvm.p, df.train)
calc.rates(yhat.train, ytrain)

# test error
yhat = predict(bestsvm.p, df.test)
calc.rates(yhat, ytest)
```
\begin{center}\textbf{Figure 3.4 - Results from tuned polynomial SVM}\end{center}
\hrulefill

```{r}
# 8. Overall, which approach seems to give the best results on this data?

rocplot = function(yhat, ytest, ...){
  predob = prediction(yhat, ytest)
  perf = performance(predob, "tpr", "fpr")
  plot(perf, ...)
  cat('\n')
}

auroc = function(yhat, ytest, kernel, ...){
  predob = prediction(yhat, ytest)
  auc = performance(predob, measure='auc')
  cat('AUC for', kernel, ':', auc@y.values[[1]])
  cat('\n')
}

fitted.l = attributes(predict(bestsvm.l, df[-train,], decision.values=T))$decision.values
rocplot(fitted.l, df[-train,'Purchase'], main="SVM, test data")

fitted.r = attributes(predict(bestsvm.r, df[-train,], decision.values=T))$decision.values
rocplot(fitted.r, df[-train,'Purchase'], add=T, col='blue')

fitted.p = attributes(predict(bestsvm.p, df[-train,], decision.values=T))$decision.values
rocplot(fitted.p, df[-train,'Purchase'], add=T, col='darkorange')

auroc(fitted.l, df[-train,'Purchase'], 'linear (black)')
auroc(fitted.r, df[-train,'Purchase'], 'radial (blue)')
auroc(fitted.p, df[-train,'Purchase'], 'polynomial (orange)')
```
\begin{center}\textbf{Figure 4 - ROC plots and AUROC for all SVMs}\end{center}
\hrulefill

# 4. Conclusions

In this assignment, I fit untuned SVMs using linear, radial, and polynomial kernels, then tuned them using 10-fold cross-validation and reported confusion matrices and results. I found that radial and polynomial SVMs performed quite poorly when `cost` was set very low, but the tuned models all performed similarly well, with radial barely beating the other two in terms of test set performance. In all three cases, a higher value of `cost` allowed for more misclassifications on the training data, which resulted in a lower variance, higher bias classifier that performed better on the test data, and that depended on fewer support vectors for its decision boundary. 

# 5. Appendix

```{r ref.label=knitr::all_labels(), echo=T, eval=F}
```